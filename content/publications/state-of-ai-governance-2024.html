<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-01">

<title>State of AI Governance, 2024 – Takshashila Institution</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/images/logo-symbol.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../assets/images/main-logo-dark.svg" alt="Takshashila Institution" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-policy-research" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Policy Research</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-policy-research">    
        <li>
    <a class="dropdown-item" href="../../pages/research_areas">
 <span class="dropdown-text">Research Areas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/latest_analysis.html">
 <span class="dropdown-text">Latest Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/newsletters.html">
 <span class="dropdown-text">Newsletters</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about-us" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">About Us</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-about-us">    
        <li>
    <a class="dropdown-item" href="../../pages/team_takshashila.html">
 <span class="dropdown-text">Team</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/mission.html">
 <span class="dropdown-text">Mission</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/milestones.html">
 <span class="dropdown-text">Milestones</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../#">
 <span class="dropdown-text">Careers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../#">
 <span class="dropdown-text">Donate</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../#"> 
<span class="menu-text">Engage with Us</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../#"> 
<span class="menu-text">Careers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../#"> 
<span class="menu-text">Donate</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
        <h2 id="toc-title">On this page</h2>
     
    
    <!-- Progress indicator container -->
    <div class="toc-progress-container">
      <div class="toc-progress-bar" id="toc-progress"></div>
    </div>
    
    <ul class="collapse">
    <li><a href="#authors" id="toc-authors" class="nav-link active" data-scroll-target="#authors">Authors</a></li>
    <li><a href="#about-the-state-of-ai-governance-report" id="toc-about-the-state-of-ai-governance-report" class="nav-link" data-scroll-target="#about-the-state-of-ai-governance-report">About the State of AI Governance Report</a></li>
    <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link" data-scroll-target="#executive-summary">Executive Summary</a></li>
    <li><a href="#timeline-of-ai-governance-events" id="toc-timeline-of-ai-governance-events" class="nav-link" data-scroll-target="#timeline-of-ai-governance-events">Timeline of AI Governance Events</a></li>
    <li><a href="#analysis-of-ai-governance-measures-across-countries" id="toc-analysis-of-ai-governance-measures-across-countries" class="nav-link" data-scroll-target="#analysis-of-ai-governance-measures-across-countries">Analysis of AI Governance Measures Across Countries</a></li>
    <li><a href="#analysis-of-ai-governance-measures-across-companies" id="toc-analysis-of-ai-governance-measures-across-companies" class="nav-link" data-scroll-target="#analysis-of-ai-governance-measures-across-companies">Analysis of AI Governance Measures Across Companies</a></li>
    <li><a href="#analysis-of-ai-governance-measures-across-multistakeholder-groupings" id="toc-analysis-of-ai-governance-measures-across-multistakeholder-groupings" class="nav-link" data-scroll-target="#analysis-of-ai-governance-measures-across-multistakeholder-groupings">Analysis of AI Governance Measures Across Multistakeholder Groupings</a></li>
    <li><a href="#predictions" id="toc-predictions" class="nav-link" data-scroll-target="#predictions">Predictions</a></li>
    <li><a href="#acronyms" id="toc-acronyms" class="nav-link" data-scroll-target="#acronyms">Acronyms</a></li>
    <li><a href="#acknowledgements-and-disclosure" id="toc-acknowledgements-and-disclosure" class="nav-link" data-scroll-target="#acknowledgements-and-disclosure">Acknowledgements and Disclosure</a></li>
    </ul>
  </nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="title-block">
    <div class="title-banner"></div>
        <div class="title-block-container">
        <h1>State of AI Governance, 2024</h1>
                <div class="title-publishers">
                        <div class="title-publisher">
                <a href=".html">
                    <h3>
                        
                    </h3>
                </a>
            </div>
                    </div>
                        <h4>April 1, 2025</h4>
                                <div class="title-categories">
                        <div class="title-category">
                <a href="../../pages/latest_analysis.html#category=AI">
                    AI
                </a>
            </div>
                        <div class="title-category">
                <a href="../../pages/latest_analysis.html#category=Governance">
                    Governance
                </a>
            </div>
                        <div class="title-category">
                <a href="../../pages/latest_analysis.html#category=Policy">
                    Policy
                </a>
            </div>
                    </div>
            </div>
    </header>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        const titleBlock = document.getElementById("title-block-header");
        const contentBlock = document.getElementById("quarto-content");

        if (titleBlock && contentBlock) {
            contentBlock.parentNode.insertBefore(titleBlock, contentBlock);
        }
    });
</script>


  
  <style>
  </style>
  
  <script>
  document.addEventListener('DOMContentLoaded', function() {
    // Get the table of contents and progress bar
    const toc = document.getElementById('TOC');
    const progressBar = document.getElementById('toc-progress');
    
    if (!toc || !progressBar) return;
    
    // Track scroll position and update the active section and progress bar
    updateTocOnScroll();
    
    function updateTocOnScroll() {
      // Get all section headings with IDs
      const headings = Array.from(document.querySelectorAll('h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]'));
      if (headings.length === 0) return;
      
      // Get all links in the TOC
      const links = Array.from(toc.querySelectorAll('a'));
      
      // Function to update active section based on scroll position
      function updateActiveSection() {
        // Calculate how far down the page has been scrolled
        const scrollPosition = window.scrollY;
        const windowHeight = window.innerHeight;
        const documentHeight = document.documentElement.scrollHeight;
        const scrollPercentage = scrollPosition / (documentHeight - windowHeight);
        
        // Update progress bar height
        progressBar.style.height = Math.min(scrollPercentage * 100, 100) + '%';
        
        // Find the current section
        let currentSection = null;
        
        for (let i = 0; i < headings.length; i++) {
          const heading = headings[i];
          const rect = heading.getBoundingClientRect();
          
          // Consider a heading in view if it's within 150px of the top
          if (rect.top <= 150) {
            currentSection = heading;
          } else {
            break;
          }
        }
        
        // Update active class on TOC links
        if (currentSection) {
          const id = currentSection.getAttribute('id');
          
          // Remove active class from all list items
          toc.querySelectorAll('li').forEach(li => li.classList.remove('active'));
          
          // Add active class to corresponding list item
          const activeLink = toc.querySelector(`a[href="#"]`);
          if (activeLink) {
            // Add active class to the parent list item
            const activeLi = activeLink.closest('li');
            if (activeLi) {
              activeLi.classList.add('active');
            }
          }
        }
      }
      
      // Listen for scroll events
      window.addEventListener('scroll', updateActiveSection);
      
      // Initialize on page load
      updateActiveSection();
    }
  });
  </script>
<section id="authors" class="level2">
<h2 data-anchor-id="authors">Authors</h2>
<p>Bharath Reddy, Rijesh Panicker, Sridhar Krishna, Arindam Goswami, Anwesha Sen, Adya Madhavan</p>
<p>Takshashila Report 2025-10. Version 1.0, April 2025.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src=".png" class="img-fluid figure-img"></p>
<figcaption>Takshashila Institution</figcaption>
</figure>
</div>
</section>
<section id="about-the-state-of-ai-governance-report" class="level2">
<h2 data-anchor-id="about-the-state-of-ai-governance-report">About the State of AI Governance Report</h2>
<p>The rapid progression in artificial intelligence capabilities has far-reaching impacts. It has the potential to significantly boost economic productivity, disrupt labour markets, and alter the balance of power between countries. The widespread diffusion of this general-purpose technology presents significant governance challenges that are amplified by the ongoing geopolitical competition for AI dominance.</p>
<p>It is against this backdrop that the Takshashila Institution, an independent centre for research and education in public policy, presents its inaugural State of AI Governance Report. This report provides a systematic comparative analysis of AI governance approaches across different countries, revealing their strategic priorities. Further, the effectiveness of corporate self-regulation initiatives, and the progress of multistakeholder collaborative efforts are also analysed. It concludes by offering predictions in these areas in the coming year.</p>
<p>This annual report will track key developments, analyse trends and offer informed predictions for the AI governance environment. It is intended to provide policymakers, analysts, and interested citizens with insights that help navigate the evolving AI governance landscape. You can learn more about our work at <a href="https://takshashila.org.in/">takshashila.org.in/</a></p>
</section>
<section id="executive-summary" class="level2">
<h2 data-anchor-id="executive-summary">Executive Summary</h2>
<p>This report analyses AI governance in three contexts – countries, companies and multistakeholder gatherings.</p>
<section id="countries" class="level3">
<h3 data-anchor-id="countries">Countries</h3>
<ul>
<li>Being at the forefront of AI innovation, the US favours a pro-market regulatory environment while prioritising geopolitical considerations to maintain its competitive advantage.</li>
<li>The EU has focused on creating a comprehensive regulatory framework that prioritises transparency, accountability, and the protection of individual rights.</li>
<li>China’s approach prioritises national security and favours heavy state control in enforcing regulations and driving innovation.</li>
<li>India has opted for a light-touch regulatory environment while investing in developing Indigenous AI models for Indian use cases.</li>
</ul>
</section>
<section id="companies" class="level3">
<h3 data-anchor-id="companies">Companies</h3>
<ul>
<li>Many companies are proactively establishing principles, guardrails and transparency and disclosure norms that guide how they build or use AI. These initiatives sometimes go beyond what is strictly expected by the regulatory environment in which they operate and are intended to build trust with users.</li>
<li>However, reporting on AI governance efforts by companies is not standardised, and the details of specific initiatives vary considerably. It is also unclear to what extent these efforts involve meaningful external scrutiny. The report analyses the governance initiatives of a few companies operating at different parts of the AI value chain.</li>
</ul>
</section>
<section id="multistakeholder-gatherings" class="level3">
<h3 data-anchor-id="multistakeholder-gatherings">Multistakeholder Gatherings</h3>
<ul>
<li>Various multistakeholder gatherings, such as the AI Summits and the Global Partnership on AI, have been formed to raise awareness and coordinate AI governance efforts among different countries.</li>
<li>Although most of these groupings do not have binding commitments or backing from all members (for instance, the US and EU refusing refused to sign the declaration on inclusive and sustainable AI at the AI Action Summit in February 2025). However, they serve as a platform to highlight important concerns and drive convergence in AI governance efforts.</li>
</ul>
<p>Finally, the report ends with some predictions on what we can expect in AI governance in the coming year.</p>
</section>
</section>
<section id="timeline-of-ai-governance-events" class="level2">
<h2 data-anchor-id="timeline-of-ai-governance-events">Timeline of AI Governance Events</h2>
<p>A timeline of significant AI governance events across countries, companies, and multistakeholder groupings is presented below. The timeline focuses only on AI Governance events and does not list milestones related to advancements in the technology.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/timeline.png" class="img-fluid figure-img"></p>
<figcaption>Timeline of AI Governance Events</figcaption>
</figure>
</div>
</section>
<section id="analysis-of-ai-governance-measures-across-countries" class="level2">
<h2 data-anchor-id="analysis-of-ai-governance-measures-across-countries">Analysis of AI Governance Measures Across Countries</h2>
<p>AI governance measures often address multiple objectives. These include ensuring transparency and accountability, promoting innovation, addressing geopolitical considerations, enabling state capacity to implement the measures, and promoting societal well-being.</p>
<p>The authors have analysed and compared the country-specific AI governance measures across these different criteria. There is some subjectivity in this comparative analysis, but the authors feel it is a useful representation of how countries are pursuing these different AI governance priorities.</p>
<p>The United States of America, the European Union, China, and India are selected as countries/regions for comparison. These have been chosen for their significant role in influencing the path of innovation, governance or adoption of AI.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/country_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Comparison of AI Governance Measures Across Countries</figcaption>
</figure>
</div>
<p>A description of the different criteria is provided below:</p>
<ul>
<li><p><strong>Transparency and Accountability</strong>: Assesses the extent to which governance frameworks attempt to promote transparency and accountability. This includes examining governance instruments such as evaluation and disclosure requirements, licensing requirements, penalties for non-compliance, and grievance redressal mechanisms.</p></li>
<li><p><strong>Promotion of Innovation</strong>: Evaluates how regulations foster innovation by creating an enabling environment. This includes examining the quantum of funding for AI infrastructure, restrictions on market participation, education and skilling initiatives and maturity of the R&amp;D ecosystem.</p></li>
<li><p><strong>Geopolitical Considerations</strong>: This assesses the extent to which policy decisions address geopolitical priorities. It includes assessing whether a state can secure access to the building blocks of AI and deny access to other countries. Relevant policy measures include export controls, investments in domestic infrastructure, promotion of open-source technologies, and policies that reduce vulnerabilities in the value chain.</p></li>
<li><p><strong>Societal Wellbeing</strong>: Assesses how regulations address broader societal concerns, such as protecting individuals from risks and harms from the adoption of AI in various sectors, and reducing environmental costs associated with AI.</p></li>
<li><p><strong>State Capacity to Govern</strong>: An estimation of the financial resources, institutional frameworks, and skilled human capital being created to enforce compliance with AI regulations effectively.</p></li>
</ul>
<section id="analysis-of-ai-governance-measures-in-the-u.s." class="level3">
<h3 data-anchor-id="analysis-of-ai-governance-measures-in-the-u.s.">Analysis of AI Governance Measures in the U.S.</h3>
<ul>
<li>The US regulatory approach utilizes existing regulatory capacity, supplementing it with AI-specific considerations.</li>
<li>Federal AI regulation in the US involves executive orders setting goals, agency rulemaking enacting specifics (such as by the NIST, DOE, DOJ and HHS), and independent agency actions within their sectors.</li>
<li>Several US states have implemented their own AI-specific regulations focusing on consumer protection, privacy, transparency, and algorithmic discrimination.</li>
<li>President Trump’s Executive Order on Removing Barriers to American Leadership in AI has revoked President Biden’s EO 14110, that might lead to changes in federal AI regulations.</li>
</ul>
<section id="transparency-accountability" class="level4">
<h4 data-anchor-id="transparency-accountability">Transparency &amp; Accountability</h4>
<ul>
<li>Compared to the EU, the US currently scores lower on transparency and accountability in AI governance partly due to the revocation of EO 14110 that had mandated disclosure for large foundational models and tasked NIST with developing safety testing standards.</li>
<li>The OMB Act mandates risk management standards for AI use in identified high-risk government systems. The DOE is evaluating tools for identifying AI model risks in critical areas like nuclear and biological threats.</li>
</ul>
</section>
<section id="promotion-of-innovation" class="level4">
<h4 data-anchor-id="promotion-of-innovation">Promotion of Innovation</h4>
<ul>
<li>The US strongly focuses on promoting AI innovation, with President Trump’s executive order aiming to remove barriers to global AI dominance.</li>
<li>Some initiatives from EO 14110 are expected to continue, such as NIST’s NAIRR project and the OMB mandate for open-sourcing government AI models and data by default.</li>
<li>The DOE will likely continue streamlining approvals for allied AI infrastructure like power and data centers.</li>
<li>NIST’s risk management framework and the US Patent Office’s guidance on AI patentability will drive standardization and innovation.</li>
</ul>
</section>
<section id="geopolitical-considerations" class="level4">
<h4 data-anchor-id="geopolitical-considerations">Geopolitical Considerations</h4>
<ul>
<li>Geopolitics is a significant focus in US AI regulation due to its leadership and competition with China.</li>
<li>The US aims to maintain a competitive edge by controlling access to AI chips through the AI diffusion framework.</li>
<li>A monitoring regime is in place for large AI model training runs on US Infrastructure as a Service (IaaS), including reporting by international customers.</li>
<li>The US Treasury prohibits certain financial transactions involving high-risk AI systems.</li>
<li>Security agencies are tasked with identifying and mitigating vulnerabilities across the AI supply chain.</li>
</ul>
</section>
<section id="societal-wellbeing" class="level4">
<h4 data-anchor-id="societal-wellbeing">Societal Wellbeing</h4>
<ul>
<li>Several US states have enacted AI-specific regulations to prevent algorithmic discrimination, ensure disclosure, and label AI-generated content.</li>
<li>State privacy laws have been updated to address the use of personal information and risks related to AI profiling and consent.</li>
<li>Federal regulations from OMB and HHS establish standards for AI use in delivering public benefits and within government.</li>
</ul>
</section>
<section id="state-capacity-to-govern" class="level4">
<h4 data-anchor-id="state-capacity-to-govern">State Capacity to Govern</h4>
<ul>
<li>The revocation of EO 14110 creates uncertainty around current US AI governance rules.</li>
<li>Regulations focusing on building AI infrastructure, capital flows, and talent are expected to remain relatively stable.</li>
<li>Regulations concerning AI governance capacity building, transparency, and auditability are likely to be reviewed and potentially rolled back.</li>
</ul>
</section>
</section>
<section id="analysis-of-ai-governance-measures-in-the-e.u." class="level3">
<h3 data-anchor-id="analysis-of-ai-governance-measures-in-the-e.u.">Analysis of AI Governance Measures in the E.U.</h3>
<ul>
<li>The EU AI Act establishes comprehensive regulatory guidelines for AI systems operating within the European Union.</li>
<li>It interacts with other regulations like the GDPR, DMA, DSA, Chips Act, and Cyber Resilience Act, that collectively influence the operations entities involved in the manufacture, deployment, import or distribution of AI systems.</li>
</ul>
<section id="transparency-accountability-1" class="level4">
<h4 data-anchor-id="transparency-accountability-1">Transparency &amp; Accountability</h4>
<ul>
<li>The EU has the most comprehensive measures for AI transparency and accountability, including risk tiering, evaluations, disclosure, licensing, penalties, and input controls.</li>
<li>AI systems in the EU are classified into unacceptable, high, limited, and minimal risk categories, with corresponding compliance obligations.</li>
</ul>
</section>
<section id="promotion-of-innovation-1" class="level4">
<h4 data-anchor-id="promotion-of-innovation-1">Promotion of Innovation</h4>
<ul>
<li>While compliance with the comprehensive regulations creates hurdles, there are provisions for promoting standards and regulatory sandboxes to facilitate adherence.</li>
<li>The InvestAI initiative aims to mobilise $216 bn for open and collaborative development of complex AI models in Europe.</li>
<li>The EU AI Act includes carve-outs to lessen the impact of penalties on SMEs and startups.</li>
</ul>
</section>
<section id="geopolitical-considerations-1" class="level4">
<h4 data-anchor-id="geopolitical-considerations-1">Geopolitical Considerations</h4>
<ul>
<li>Strategic autonomy and technology sovereignty have gained prominence in EU AI policy discussions.</li>
<li>The InvestAI initiative funds the open development of complex AI models, including AI gigafactories for data centers and the creation of datasets.</li>
<li>The EU Chips Act involves significant investment to enhance competitiveness and resilience in the semiconductor industry.</li>
<li>The EU AI Act includes exceptions for AI models released under free and open-source licenses.</li>
</ul>
</section>
<section id="societal-wellbeing-1" class="level4">
<h4 data-anchor-id="societal-wellbeing-1">Societal Wellbeing</h4>
<ul>
<li>Among the regions compared in this report, the EU has the most comprehensive measures to address societal wellbeing through various governance instruments such as tiering, evaluation and performance requirements, disclosure mandates, licensing and certifications, penalties and input controls</li>
<li>Voluntary codes of conduct are encouraged for assessing and minimising the environmental impact of AI systems.</li>
</ul>
</section>
<section id="state-capacity-to-govern-1" class="level4">
<h4 data-anchor-id="state-capacity-to-govern-1">State Capacity to Govern</h4>
<ul>
<li>The EU AI Act mandates the creation of governance institutions at both the EU and member state levels for implementation.</li>
<li>This includes the establishment of the EU AI Office, advised by a scientific panel of independent experts.</li>
<li>Member states are required to establish their own institutions to enforce compliance with the EU AI Act regulations.</li>
</ul>
</section>
</section>
<section id="analysis-of-ai-governance-measures-in-china" class="level3">
<h3 data-anchor-id="analysis-of-ai-governance-measures-in-china">Analysis of AI Governance Measures in China</h3>
<ul>
<li>China’s AI governance framework is a comprehensive, state-driven approach across multiple dimensions.</li>
<li>The system prioritizes centralized control through proactive regulation requiring algorithm transparency, security assessments, ethics reviews, and content monitoring aligned with state objectives.</li>
<li>China’s governance model emphasizes public-private collaboration for AI innovation along with strategic state investments.</li>
<li>China’s governance strategy strongly reflects national security priorities through data sovereignty measures, indigenous computing development and military-civil fusion initiatives.</li>
<li>Massive financial investments such as $140 bn from the Bank of China and $184 bn in government VC funding are announced for AI firms.</li>
</ul>
<section id="transparency-accountability-2" class="level4">
<h4 data-anchor-id="transparency-accountability-2">Transparency &amp; Accountability</h4>
<ul>
<li>China’s state-enforced AI regulations demonstrate a preference for proactive, centralised governance.</li>
<li>Chinese regulations mandate algorithm filing, transparency, security assessments, and ethics reviews before AI deployment.</li>
<li>Watermarking for AI-generated content and complaint redressal mechanisms are also required in China.</li>
</ul>
</section>
<section id="promotion-of-innovation-2" class="level4">
<h4 data-anchor-id="promotion-of-innovation-2">Promotion of Innovation</h4>
<ul>
<li>China’s governance mechanisms focus on state-driven public-private collaboration and strategic infrastructure development for innovation.</li>
<li>There is a strong emphasis on AI talent cultivation and public-private research partnerships in China, including innovation hubs supported by AI-focused education guidelines and university-affiliated research centers.</li>
<li>Guidelines reinforce an open AI ecosystem, promoting resource-sharing, interoperability, and common technical standards.</li>
<li>Large-scale infrastructure projects ensure regional computing power distribution and provide tax incentives for AI infrastructure investments.</li>
</ul>
</section>
<section id="geopolitical-considerations-2" class="level4">
<h4 data-anchor-id="geopolitical-considerations-2">Geopolitical Considerations</h4>
<ul>
<li>Policies emphasise technological self-sufficiency through measures requiring data sovereignty, AI security, indigenous computing infrastructure development and military-civil fusion.</li>
<li>Strict data localization measures are imposed, requiring government approval for cross-border transfers of “important data.”</li>
<li>Mandatory algorithmic filing and real-time content monitoring ensure AI-generated content aligns with state narratives.</li>
<li>Real-name verification requirements under cybersecurity laws address information control and foreign influence concerns.</li>
</ul>
</section>
<section id="societal-wellbeing-2" class="level4">
<h4 data-anchor-id="societal-wellbeing-2">Societal Wellbeing</h4>
<ul>
<li>A state-led approach focuses on AI ethics, data privacy, content moderation, and digital fairness.</li>
<li>Explicit user consent for personal data usage and mandatory opt-out options for personalised recommendations are required.</li>
<li>Algorithmic price discrimination is banned, and watermarking/labeling of AI-generated content is mandated.</li>
<li>Explicit user consent is required for use of voices and images in synthetic media.</li>
</ul>
</section>
<section id="state-capacity-to-govern-2" class="level4">
<h4 data-anchor-id="state-capacity-to-govern-2">State Capacity to Govern</h4>
<ul>
<li>China’s capacity to govern AI development is demonstrated through massive financial investments and centralised infrastructure planning.</li>
<li>The government directs significant capital into priority AI sectors at an unprecedented scale.</li>
<li>Local governments have also dedicated billions to AI research and infrastructure, reflecting a decentralised but state-coordinated funding approach.</li>
<li>Strong oversight is provided by government institutions like the Cyberspace Administration of China (CAC).</li>
<li>The state has the capacity to fund and manage large-scale computing networks for AI development.</li>
</ul>
</section>
</section>
<section id="analysis-of-ai-governance-measures-in-india" class="level3">
<h3 data-anchor-id="analysis-of-ai-governance-measures-in-india">Analysis of AI Governance Measures in India</h3>
<ul>
<li>India’s AI governance is characterised by limited regulations, with sector-specific guidelines in finance and health.</li>
<li>The government is promoting AI innovation through the IndiaAI Mission addressing priority use cases such as agriculture, education and healthcare.</li>
<li>Some policy interventions aim to address vulnerabilities related to access to AI resources.</li>
<li>There is a lack of comprehensive regulations in India for managing the broad risks associated with AI.The draft AI Governance Guidelines propose a Technical Secretariat to enhance state capacity to govern AI, but its implementation is uncertain.</li>
</ul>
<section id="transparency-accountability-3" class="level4">
<h4 data-anchor-id="transparency-accountability-3">Transparency &amp; Accountability</h4>
<ul>
<li>India has limited cross-sector regulations. The finance and health sectors in India have more specific regulations and AI safety guidelines.The RBI has formed a panel to review AI regulation in finance, identify risks, and recommend a compliance framework. Similarly, ICMR Ethical Guidelines provide principles for AI development and deployment in healthcare.</li>
<li>The AI Advisory (2024) recommends labeling deepfake and synthetic content, and draft guidelines suggest voluntary incident reporting.</li>
</ul>
</section>
<section id="promotion-of-innovation-3" class="level4">
<h4 data-anchor-id="promotion-of-innovation-3">Promotion of Innovation</h4>
<ul>
<li>The IndiaAI Mission has allocated over $1.2 bn to develop AI models, datasets, compute, and education.</li>
<li>Efforts are underway to enable access to compute resources and datasets through portals like IndiaAI Compute and AIKosha.</li>
<li>While aiming to encourage startups and MSMEs, the implementation of these innovation initiatives may face inefficiencies.</li>
</ul>
</section>
<section id="geopolitical-considerations-3" class="level4">
<h4 data-anchor-id="geopolitical-considerations-3">Geopolitical Considerations</h4>
<ul>
<li>The US AI Diffusion Framework’s export controls could potentially restrict India’s access to computing resources and advanced AI models.</li>
<li>India is attempting to mitigate these risks by promoting indigenous AI models and domestic computing clusters.</li>
</ul>
</section>
<section id="societal-wellbeing-3" class="level4">
<h4 data-anchor-id="societal-wellbeing-3">Societal Wellbeing</h4>
<ul>
<li>Sector-specific guidelines from ICMR and RBI, along with the AI Advisory (2024), offer a voluntary ethical framework for societal well-being in AI.</li>
<li>There are currently no overarching regulations in India to address the diverse risks posed by AI technologies.</li>
</ul>
</section>
<section id="state-capacity-to-govern-3" class="level4">
<h4 data-anchor-id="state-capacity-to-govern-3">State Capacity to Govern</h4>
<ul>
<li>MeitY’s draft AI Governance Guidelines propose a Technical Secretariat to monitor and mitigate AI risks and harms in real-time, serving as a bridge between the industry and policymakers.</li>
<li>As these are draft guidelines, it remains to be seen how state capacity will be enhanced to implement them.</li>
</ul>
</section>
</section>
</section>
<section id="analysis-of-ai-governance-measures-across-companies" class="level2">
<h2 data-anchor-id="analysis-of-ai-governance-measures-across-companies">Analysis of AI Governance Measures Across Companies</h2>
<p>Companies are proactively adopting AI governance measures. These measures include developing AI principles, implementing risk mitigation strategies, enhancing transparency and establishing governance structures.</p>
<p>However, there is currently a lack of standardization in how companies report on their AI governance efforts. The specific details of company AI governance initiatives vary considerably.</p>
<p>The extent of meaningful external scrutiny of company AI governance efforts is unclear. It is also challenging to understand how companies are operationalizing their principles, measuring the effectiveness of their risk mitigation strategies, and ensuring accountability.</p>
<p>Microsoft, Google, OpenAI, Mistral, Anthropic, Amazon, Accenture, and Deloitte are selected for the comparative analysis. These companies operate across different stages of the AI value chain, including big technology platforms, AI model developers, and technology services firms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/company_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Analysis of AI Governance Measures Across Companies</figcaption>
</figure>
</div>
<section id="principles" class="level3">
<h3 data-anchor-id="principles">Principles</h3>
<p>The extent to which the organisation’s responsible AI policy is articulated and identifies the principles it seeks to adhere to. A comparison of the articulation of principles on responsible AI development and use by different companies is provided below.</p>
<ul>
<li><p><strong>Microsoft</strong>: In 2018, six ethical principles were adopted: accountability, transparency, fairness, reliability and safety, privacy and security, and inclusiveness. These principles have been translated into corporate policies, including a standard for engineering teams.</p></li>
<li><p><strong>Google</strong>: First published in 2018 and updated recently, the principles are bold innovation, responsible development and deployment, and collaborative progress.</p></li>
<li><p><strong>OpenAI</strong>: Committed to building safe and beneficial AGI. Specific principles are not detailed. Its usage policy prohibits uses such as biometric profiling, deceptive techniques, harm to children, etc.</p></li>
<li><p><strong>Mistral</strong>: Mistral’s responsible AI policy emphasises key principles such as neutrality, empowerment, building trust and minimising potential harm and misuse. Furthermore, its terms of use emphasise data privacy and security and restrict access to individuals under 13.</p></li>
<li><p><strong>Anthropic</strong>: Anthropic emphasizes proportional safeguards that scale with the potential risks of AI systems. Their approach is inspired by biosafety standards, ensuring that safety measures align with the capabilities of their models.</p></li>
<li><p><strong>Amazon</strong>: AWS has introduced an updated Responsible AI Policy in January, 2025, which supplements the AWS Acceptable Use Policy and AWS Service Terms. This policy outlines a set of prohibited actions, and includes commitments to developing safe, fair, and accurate AI and machine learning services.</p></li>
<li><p><strong>Accenture</strong>: Accenture’s four pillars of responsible AI implementation are: Organizational, Operational, Technical, and Reputational.</p></li>
<li><p><strong>Deloitte</strong>: Uses a Trustworthy AI™ framework to help organizations develop ethical safeguards across seven key dimensions: transparent and explainable, fair and impartial, robust and reliable, respectful of privacy, safe and secure, responsible, and accountable.</p></li>
</ul>
</section>
<section id="risk-mitigation" class="level3">
<h3 data-anchor-id="risk-mitigation">Risk Mitigation</h3>
<p>The extent to which policy identifies the potential risks and lists actions to mitigate against those risks. The risk mitigation efforts by different companies are listed below:</p>
<ul>
<li><p><strong>Microsoft</strong>: AI risk management is part of the company’s enterprise risk management system. They also prioritise development of open-access responsible AI tools to map, measure and manage risks.</p></li>
<li><p><strong>Google</strong>: Work with external organizations to establish boundaries and reduce the risks of abuse. Google has developed a frontier safety framework to prepare for risks from frontier models. They have also published a generative AI toolkit and people + AI guidebook that guide AI development and deployment.</p></li>
<li><p><strong>OpenAI</strong>: Investments in research to inform regulation – including techniques for assessing potentially dangerous capabilities. Their red-teaming and safety procedures are publicly disclosed.</p></li>
<li><p><strong>Mistral</strong>: Includes measures that address data privacy and security. It prohibits use for illegal activities, hate and discrimination, misinformation, and professional advice. They have a zero-tolerance policy regarding child sexual abuse material.</p></li>
<li><p><strong>Anthropic</strong>: They employ AI Safety Level (ASL) Standards to address catastrophic risks, such as misuse or unintended autonomous actions. These standards include rigorous testing and safeguards before deploying advanced models.</p></li>
<li><p><strong>Amazon</strong>: Prohibits use for disinformation, deception, violation of privacy, harm or abuse of minors. Also prohibits AI/ML services for circumvention of safety filters and use in a weapon without human intervention.</p></li>
<li><p><strong>Accenture</strong>: Ongoing testing of AI for human impact, fairness, explainability, transparency, accuracy, and safety. Employing use of state-of-the-art responsible AI tools and technologies to mitigate problems.</p></li>
<li><p><strong>Deloitte</strong>: Using tools to test and monitor AI models.</p></li>
</ul>
</section>
<section id="transparency-and-reporting" class="level3">
<h3 data-anchor-id="transparency-and-reporting">Transparency and Reporting</h3>
<p>The extent to which governance frameworks attempt to promote transparency and accountability. The measures by companies that promote transparency and accountability are listed below.</p>
<ul>
<li><p><strong>Microsoft</strong>: Transparency reports are published with recent reports aligned with the NIST framework. Provide documentation on the intended use, limitations and risks of AI technologies to consumers.</p></li>
<li><p><strong>Google</strong>: Transparency reports are published with recent reports aligned with the NIST framework. Gemini app, Google Cloud and Google Workspace are ISO/IEC 42001 certified. Engaging with governments, civil society, and academia on information sharing, establishing common standards, and promoting best practices for AI safety.</p></li>
<li><p><strong>OpenAI</strong>: Transparency reports and system cards for different products detail the red-teaming and safety procedures.</p></li>
<li><p><strong>Mistral</strong>: Publishes documentation for its AI models with details on their capabilities and limitations. Provides channels for reporting incidents, contact information for inquiries, and communities for users.</p></li>
<li><p><strong>Anthropic</strong>: Anthropic commits to sharing updates on their governance practices and lessons learned. They aim to maintain public trust by openly communicating their safety measures and policies.</p></li>
<li><p><strong>Amazon</strong>: Provides details on how AI systems are deployed, monitored and managed during their development and operations. Mentions openly sharing development choices, including data sources and algorithms.</p></li>
<li><p><strong>Accenture</strong>: Public disclosure of AI systems’ capabilities, limitations, and suitable uses, addressing both security and societal risks.</p></li>
<li><p><strong>Deloitte</strong>: Recommends engaging stakeholders in AI governance by defining clear roles and responsibilities, documenting accountabilities, establishing expectations for AI ethics and trust, and empowering employees to voice concerns and act ethically.</p></li>
</ul>
</section>
<section id="governance-structure" class="level3">
<h3 data-anchor-id="governance-structure">Governance Structure</h3>
<p>An estimation of the financial resources, institutional frameworks, and skilled human capital made available to enforce compliance with AI regulations effectively. A comparison of the governance structure of different companies is provided below.</p>
<ul>
<li><p><strong>Microsoft</strong>: Aether is an internal advisory body set up to focus on AI ethics and responsible AI practices within the company. In 2023, over 350 employees worked on responsible AI, developing best practices for building safe, secure, and transparent AI systems designed to benefit society.</p></li>
<li><p><strong>Google</strong>: The AGI Safety Council and the Responsibility and Safety Council oversee alignment with AI principles. Google forced AI researcher Dr.&nbsp;Gebru to resign after she resisted orders to halt research indicating that speech technology, like Google’s, could negatively impact marginalized groups. Others also resigned in response to how Dr.&nbsp;Gebru was treated.</p></li>
<li><p><strong>OpenAI</strong>: Specific governance structures are not publicly available. Concrete governance practices are specifically tailored to highly capable foundation models. The company structure has also seen a lot of churn, changing from a non-profit to a “capped profit” structure to enable the pursuit of AGI. Former employees have also raised concerns about the safety practices at the company.</p></li>
<li><p><strong>Mistral</strong>: Does not provide any details on the governance structure.</p></li>
<li><p><strong>Anthropic</strong>: Their governance framework is iterative and adaptable, incorporating lessons from high-consequence industries. It includes internal evaluations and external inputs to refine their policies.</p></li>
<li><p><strong>Amazon</strong>: Does not provide any details on the governance structure.</p></li>
<li><p><strong>Accenture</strong>: Establishment of transparent governance structures across domains with defined roles, expectations, and accountability. Creation cross-domain ethics committees. Has establishment a Chief Responsible AI officer role.</p></li>
<li><p><strong>Deloitte</strong>: Evaluating roles and responsibilities and implementing change management and training.</p></li>
</ul>
</section>
<section id="third-party-oversight" class="level3">
<h3 data-anchor-id="third-party-oversight">Third-Party Oversight</h3>
<p>The willingness to subject itself to third-party oversight. Examines policies that encourage third-party oversight to identify and report risks they might have overlooked.</p>
<ul>
<li><p><strong>Microsoft</strong>: Bug bounty programs are used to incentivise external discovery and reporting of issues and vulnerabilities. They are also building external red teaming capacity to enable third-party oversight, especially of sensitive capabilities of highly capable models.</p></li>
<li><p><strong>Google</strong>: Google Cloud AI achieved a “mature” rating in a third-party evaluation. Encourages third-party discovery and reporting of issues and vulnerabilities. Works with industry peers and standards-setting bodies towards efforts such as developing technical frameworks to help users distinguish synthetic media.</p></li>
<li><p><strong>OpenAI</strong>: Bounty systems to encourage responsible disclosure of weaknesses and vulnerabilities.</p></li>
<li><p><strong>Mistral</strong>: Its policies do not explicitly seek third-party oversight, but its commitment to transparency indicates a willingness to be open about its AI practices which could extend to third-party oversight.</p></li>
<li><p><strong>Anthropic</strong>: Anthropic collaborates with external experts for adversarial testing and red-teaming of their models. This ensures that their systems meet stringent safety and security standards.</p></li>
<li><p><strong>Amazon</strong>: While AWS is committed to developing safe, fair and accurate AI/ML services and provides tools and guidance to assist in this, it does not talk about third-party oversight.</p></li>
<li><p><strong>Accenture</strong>: No mention of third-party oversight.</p></li>
<li><p><strong>Deloitte</strong>: Offers the Omnia Trustworthy AI module, which provides guidelines and guardrails for designing, developing, deploying, and operating ethical AI solutions. Does not mention using third party oversight over their own AI solutions.</p></li>
</ul>
</section>
</section>
<section id="analysis-of-ai-governance-measures-across-multistakeholder-groupings" class="level2">
<h2 data-anchor-id="analysis-of-ai-governance-measures-across-multistakeholder-groupings">Analysis of AI Governance Measures Across Multistakeholder Groupings</h2>
<p>Various multistakeholder gatherings, including the AI Summits and the Global Partnership on AI, have been established to raise awareness and coordinate international AI governance efforts.</p>
<p>While state-level efforts have tended to focus on innovation and geopolitics, multistakeholder gatherings highlight broader societal concerns arising from the rapid development of advanced AI.</p>
<p>Most gatherings do not have legally binding commitments or backing from all members (for instance, the US and EU refusing refused to sign the declaration on inclusive and sustainable AI at the AI Action Summit in February 2025).</p>
<p>Achieving alignment or convergence on AI regulations through these platforms can simplify compliance for multinational technology companies.</p>
<p>The analysis in this section focuses on the membership composition, guiding principles, and recent developments in these gatherings.</p>
<section id="the-organization-for-economic-co-operation-and-development" class="level3">
<h3 data-anchor-id="the-organization-for-economic-co-operation-and-development">The Organization for Economic Co-operation and Development</h3>
<p><strong>Membership:</strong> - OECD has 38 member countries committed to democracy, collaborating on addressing global policy changes, and is not an AI-specific body.</p>
<p><strong>Principles and areas of focus:</strong> - OECD promotes inclusive growth, human-centric values, transparency and explainability, robustness and accountability of AI systems. - The OECD AI Principles are the first intergovernmental standard on AI.</p>
</section>
<section id="global-partnership-on-ai" class="level3">
<h3 data-anchor-id="global-partnership-on-ai">Global Partnership on AI</h3>
<p><strong>Membership:</strong> - GPAI has 44 member countries, including the US, EU, UK, Japan, and India.</p>
<p><strong>Principles and areas of focus:</strong> - GPAI promotes the responsible development of AI grounded in human rights, inclusion, diversity, innovation, and economic growth - Areas of focus include responsible AI, data governance, the future of work, and innovation and commercialisation.</p>
<p><strong>Developments:</strong> - As of 2024, GPAI and the OECD formally joined forces to combine their work on AI and implement human-centric, safe, secure, and trustworthy AI. The two bodies are committed to implementing the OECD Recommendation on Artificial Intelligence.</p>
</section>
<section id="ai-governance-alliance" class="level3">
<h3 data-anchor-id="ai-governance-alliance">AI Governance Alliance</h3>
<p><strong>Membership:</strong> - The AI governance alliance is a global initiative launched by the World Economic Forum. The alliance has over 603 members from more than 500 organisations globally.</p>
<p><strong>Principles and areas of focus:</strong> - The principles of the AI Governance Alliance include responsible and ethical AI, inclusivity, transparency, international collaboration and multistakeholder engagement. - The areas of focus include safe systems and technologies, responsible applications and transformation, resilient governance and regulation.</p>
</section>
<section id="ai-summits" class="level3">
<h3 data-anchor-id="ai-summits">AI Summits</h3>
<p><strong>Membership:</strong> - The AI summits are a series of international conferences addressing the challenges and opportunities presented by AI. Participants include heads of state and major companies such as Meta and DeepMind.</p>
<p><strong>Principles and areas of focus:</strong> - Each AI summit has set its own agenda, but some common principles are ethical AI development, safety and security, transparency and accountability, and international collaboration. - The AI Summits have been held thrice since their inception. The first summit, focussing on AI safety, was held at Bletchley Park in the UK in 2023. The second summit was held in Seoul, South Korea, in 2024. - The third event, the AI Action Summit was held in Paris in February 2025 and was attended by representatives from more than 100 countries. While 58 countries, including France, China and India, signed a joint declaration, the US and UK refused to sign the declaration on inclusive and sustainable AI. - The agenda has evolved from existential risks and global cooperation at Bletchley, to risk management frameworks and company commitments at Seoul to an action oriented focus on public interest, sustainability and global governance at Paris.</p>
</section>
<section id="united-nations" class="level3">
<h3 data-anchor-id="united-nations">United Nations</h3>
<p><strong>Membership:</strong> - The UN is an international organisation committed to global peace and security, with 193 member states, including almost all internationally-recognised sovereign states. The safe development of AI is one of their many areas of work.</p>
<p><strong>Principles and areas of focus:</strong> - Some of their core principles include doing no harm. AI applications should have a clear purpose, fairness and non-discrimination, safety and security to prevent misuse and harm, responsibility and accountability. - The UN Secretary-General is convening a multi-stakeholder High-level Advisory Body on AI to study and provide recommendations for the international governance of AI. - Other efforts include convening global dialogues, developing standards and building capacity.</p>
</section>
<section id="united-nations-educational-scientific-and-cultural-organization" class="level3">
<h3 data-anchor-id="united-nations-educational-scientific-and-cultural-organization">United Nations Educational, Scientific and Cultural Organization</h3>
<p><strong>Membership:</strong> - UNESCO is a specialised agency of the UN with 194 member states and 12 associate member states</p>
<p><strong>Principles and areas of focus:</strong> - The UNESCO general conference adopted the recommendation on the ethics of artificial intelligence – the first global standard on AI ethics principles aligned with the UN’s principles on AI. - Areas of focus include developing an AI Readiness Assessment Methodology, facilitating policy dialogues and capacity building initiatives.</p>
</section>
</section>
<section id="predictions" class="level2">
<h2 data-anchor-id="predictions">Predictions</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 25%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Confidence</th>
<th>Region</th>
<th>Prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>High</td>
<td>Global</td>
<td>Compute thresholds for enforcing regulations will no longer be relevant. The effectiveness of these thresholds might be challenged as a measure of capability as inference computing begins to scale and smaller models become more efficient. The US and EU have 10^26 and 10^25 flops as training compute thresholds for enforcing certain regulations.</td>
</tr>
<tr class="even">
<td>High</td>
<td>Global</td>
<td>Investments in sovereign cloud infrastructure will increase, driven by geopolitical considerations.</td>
</tr>
<tr class="odd">
<td>High</td>
<td>China, EU</td>
<td>Open-source and open-weight models will continue to be pushed by China and EU as a pathway to strategic autonomy and technology leadership. DeepSeek and Mistral will remain open-weight or open-source.</td>
</tr>
<tr class="even">
<td>High</td>
<td>India</td>
<td>The compute capacity created under the IndiaAI mission aimed towards incentivising startups and building indigenous models will be underutilised. This is due to lack of demand that meets the criteria to qualify for the subsidies as well as due to friction in the bureaucratic process involved.</td>
</tr>
<tr class="odd">
<td>Moderate</td>
<td>Global</td>
<td>AI governance regulations at the state level will continue to prioritise innovation over encouraging transparency, accountability, and societal well-being. In other words geopolitical considerations will trump protection of individual rights as a governance priority.</td>
</tr>
<tr class="even">
<td>Moderate</td>
<td>US</td>
<td>US chip restrictions on China will not escalate further. This is because Deep Seek makes owning newer chips less relevant.</td>
</tr>
<tr class="odd">
<td>Moderate</td>
<td>US</td>
<td>Federal laws focused on monitoring AI safety and federal agency assessment of AIs for discrimination and bias will be made defunct or watered down significantly. By the end of the year, AI safety guardrails will be driven by private firms.</td>
</tr>
<tr class="even">
<td>Moderate</td>
<td>EU</td>
<td>EU’s comprehensive regulatory framework, including penalties for non-compliance, would result in a few companies not releasing their AI models/features in the EU. This might lead to a milder enforcement of the regulations. As per the declared timeline, rules on notified bodies, general purpose AI models, governance, confidentiality, and penalties start to apply from August, 2025.</td>
</tr>
<tr class="odd">
<td>Moderate</td>
<td>China</td>
<td>The US Diffusion Framework will not stop state-of-the-art AI models coming out of China, at least not in the next year. This is because Deep Seek makes owning newer chips less relevant, and China has built up an overcapacity of data centres over the past few years.</td>
</tr>
<tr class="even">
<td>Moderate</td>
<td>India</td>
<td>Regulatory focus will be on protecting the information ecosystem from content deemed harmful for the government or Indian society.</td>
</tr>
<tr class="odd">
<td>Moderate</td>
<td>Corporate</td>
<td>Governance of AI within companies will become a bigger requirement as governments firm up on their positions regarding AI. ‘Chief Responsible AI Officer’ will be a new role at companies seeking to deploy AI solutions at scale, whose duty it will be to ensure AI is deployed in a manner that will, at the very least, protect them from litigation.</td>
</tr>
</tbody>
</table>
<p>Confidence reflects how likely the authors believe their prediction will be accurate in the coming year.</p>
</section>
<section id="acronyms" class="level2">
<h2 data-anchor-id="acronyms">Acronyms</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td>AI</td>
<td>Artificial Intelligence</td>
</tr>
<tr class="even">
<td>CAC</td>
<td>Cyberspace Administration of China</td>
</tr>
<tr class="odd">
<td>DMA</td>
<td>Digital Markets Act</td>
</tr>
<tr class="even">
<td>DOE</td>
<td>Department of Energy</td>
</tr>
<tr class="odd">
<td>DOJ</td>
<td>Department of Justice</td>
</tr>
<tr class="even">
<td>DPDPA</td>
<td>Digital Personal Data Protection Act</td>
</tr>
<tr class="odd">
<td>DSA</td>
<td>Digital Services Act</td>
</tr>
<tr class="even">
<td>EO</td>
<td>Executive Order</td>
</tr>
<tr class="odd">
<td>EU</td>
<td>European Union</td>
</tr>
<tr class="even">
<td>GDPR</td>
<td>General Data Protection Regulation</td>
</tr>
<tr class="odd">
<td>GPAI</td>
<td>Global Partnership on AI</td>
</tr>
<tr class="even">
<td>HHS</td>
<td>Health and Human Services</td>
</tr>
<tr class="odd">
<td>IaaS</td>
<td>Infrastructure as a Service</td>
</tr>
<tr class="even">
<td>ICMR</td>
<td>Indian Council of Medical Research</td>
</tr>
<tr class="odd">
<td>MeitY</td>
<td>Ministry of Electronics and Information Technology</td>
</tr>
<tr class="even">
<td>MSMEs</td>
<td>Micro, Small and Medium Enterprises</td>
</tr>
<tr class="odd">
<td>NAIRR</td>
<td>National Artificial Intelligence Research Resource</td>
</tr>
<tr class="even">
<td>NIST</td>
<td>National Institute of Standards and Technology</td>
</tr>
<tr class="odd">
<td>OECD</td>
<td>Organization for Economic Co-operation and Development</td>
</tr>
<tr class="even">
<td>OMB</td>
<td>Office of Management and Budget</td>
</tr>
<tr class="odd">
<td>RBI</td>
<td>Reserve Bank of India</td>
</tr>
<tr class="even">
<td>SMEs</td>
<td>Small and Medium-sized Enterprises</td>
</tr>
<tr class="odd">
<td>UN</td>
<td>United Nations</td>
</tr>
<tr class="even">
<td>UNESCO</td>
<td>United Nations Educational, Scientific and Cultural Organization</td>
</tr>
<tr class="odd">
<td>US</td>
<td>United States of America</td>
</tr>
<tr class="even">
<td>VC</td>
<td>Venture Capital</td>
</tr>
</tbody>
</table>
</section>
<section id="acknowledgements-and-disclosure" class="level2">
<h2 data-anchor-id="acknowledgements-and-disclosure">Acknowledgements and Disclosure</h2>
<p>The authors would like to thank their colleague, Pranay Kotasthane, for his valuable feedback and suggestions.</p>
<p>The authors also extend their appreciation to the creators of <a href="https://agora.cltc.org/">AGORA</a>, an exploration and analysis tool for AI-relevant laws, regulations, standards, and other governance documents by the Emerging Technology Observatory.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/AksharPrasanna\.github\.io\/takshashila-project\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<!-- Footer Component with Social Media Banner -->

<div class="social-banner">
    <div class="social-banner-container">
        <div class="social-text">Follow us to get the latest policy updates</div>
        <div class="social-icons">
            <a href="#" class="social-icon" aria-label="Facebook Page">
                <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 512">
                    <path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"></path>
                </svg>
            </a>
            <a href="#" class="social-icon" aria-label="Twitter Profile">
                <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 512">
                    <path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path>
                </svg>
            </a>
            <a href="#" class="social-icon" aria-label="LinkedIn Company">
                <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512">
                    <path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path>
                </svg>
            </a>
            <a href="#" class="social-icon" aria-label="Instagram Page">
                <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512">
                    <path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"></path>
                </svg>
            </a>
            <a href="#" class="social-icon" aria-label="Youtube Channel">
                <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 576 512">
                    <path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path>
                </svg>
            </a>
        </div>
    </div>
</div>

<!-- Footer Component -->
<footer class="footer">
    <div class="footer-container">
        <!-- Left Side: Organization Info -->
        <div class="footer-org-info">
            <img src="../../assets/images/main-logo-dark.svg" alt="Takshashila Institution" class="footer-logo">
            <div class="org-description">
                Connecting good people to good ideas and good networks.
            </div>
            <div class="org-address">
                The Takshashila Institution<br>
                2nd floor, 49/1, Cobalt Building, Church Street<br>
                Bengaluru, Karnataka, India - 560001
            </div>
        </div>

        <!-- Right Side: Navigation Links Grid -->
        <div class="footer-nav-grid">
            <div class="footer-nav-column">
                <h3 class="footer-heading">Think Tank</h3>
                <ul class="footer-links">
                    <li><a href="../../pages/research_areas/index.html">Research</a></li>
                    <li><a href="#">Commentaries</a></li>
                    <li><a href="#">All things policy</a></li>
                    <li><a href="#">Newsletter</a></li>
                    <li><a href="../../pages/ai_policy.html">Principles for the use of AI</a></li>
                </ul>
            </div>

            <div class="footer-nav-column">
                <h3 class="footer-heading">Education</h3>
                <ul class="footer-links">
                    <li><a href="#">Privacy Policy</a></li>
                    <li><a href="#">Open Takshashila</a></li>
                    <li><a href="#">Political Mandala</a></li>
                    <li><a href="#">Takshashila Library</a></li>
                </ul>
            </div>

            <div class="footer-nav-column">
                <h3 class="footer-heading">Legal</h3>
                <ul class="footer-links">
                    <li><a href="../../pages/ombudsman.html">Ombudsman</a></li>
                    <li><a href="../../pages/terms.html">Terms &amp; Conditions</a></li>
                    <li><a href="../../pages/privacy.html">Privacy policy</a></li>
                </ul>
            </div>

            <div class="footer-nav-column">
                <h3 class="footer-heading">About</h3>
                <ul class="footer-links">
                    <li><a href="../../pages/mission.html">Mission</a></li>
                    <li><a href="#">Milestones</a></li>
                    <li><a href="#">Careers</a></li>
                    <li><a href="#">Donate</a></li>
                </ul>
            </div>

        </div>
    </div>

    <div class="footer-bottom">
        <div class="copyright">
            Copyright © 2010-2024. The Takshashila Institution. All rights reserved.
        </div>
    </div>
</footer>




</body></html>